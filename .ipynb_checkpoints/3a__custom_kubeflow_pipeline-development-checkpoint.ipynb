{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b013d79-ffb3-473d-a873-ad0bef699a2a",
   "metadata": {},
   "source": [
    "### What we will learn\n",
    "\n",
    "- We will build custom Kubeflow pipeline for classfication\n",
    "- We will show how to use Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14bd11f9-6e75-4ee8-82fd-3141cd4a5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a2454c7-5fc4-43fa-b2f4-46ca3a7c4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.14\n"
     ]
    }
   ],
   "source": [
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a01739a-0aab-4a9d-ae9e-670f6d4df2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d90a7f6-aba3-4dde-9ad6-fbcf31c23e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGQUERY_PROJECT_ID:  datafusionsbox\n",
      "BIGQUERY_DATASET:  dataset4ccc\n",
      "BIGQUERY_DATASET_REGION:  us\n",
      "BIGQUERY_TABLE:  df_for_model_ccc_with_weights\n",
      "VERTEXAI_PROJECT_ID:  datafusionsbox\n",
      "VERTEXAI_REGION:  us-central1\n",
      "BUCKET_NAME:  gcp-demo-ccc-vertexai\n",
      "BUCKET_URI:  gcp-demo-ccc-vertexai\n",
      "BUCKET_REGION:  us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BIGQUERY_PROJECT_ID = os.environ.get('BIGQUERY_PROJECT_ID')\n",
    "BIGQUERY_DATASET = os.environ.get('BIGQUERY_DATASET')\n",
    "BIGQUERY_DATASET_REGION = os.environ.get('BIGQUERY_DATASET_REGION')\n",
    "BIGQUERY_TABLE = os.environ.get('BIGQUERY_TABLE')\n",
    "\n",
    "VERTEXAI_PROJECT_ID = os.environ.get('VERTEXAI_PROJECT_ID')\n",
    "VERTEXAI_REGION = os.environ.get('VERTEXAI_REGION')\n",
    "\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
    "BUCKET_URI = os.environ.get('BUCKET_URI')\n",
    "BUCKET_REGION = os.environ.get('BUCKET_REGION')\n",
    "\n",
    "PREFIX = os.environ.get('PREFIX')\n",
    "\n",
    "print(\"BIGQUERY_PROJECT_ID: \",BIGQUERY_PROJECT_ID)\n",
    "print(\"BIGQUERY_DATASET: \",BIGQUERY_DATASET)\n",
    "print(\"BIGQUERY_DATASET_REGION: \",BIGQUERY_DATASET_REGION)\n",
    "print(\"BIGQUERY_TABLE: \",BIGQUERY_TABLE)\n",
    "\n",
    "print(\"VERTEXAI_PROJECT_ID: \",VERTEXAI_PROJECT_ID)\n",
    "print(\"VERTEXAI_REGION: \",VERTEXAI_REGION)\n",
    "\n",
    "print(\"BUCKET_NAME: \",BUCKET_NAME)\n",
    "print(\"BUCKET_URI: \",BUCKET_NAME)\n",
    "print(\"BUCKET_REGION: \",VERTEXAI_REGION)\n",
    "\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root'.format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bc26e95-4604-4be3-8d6c-ac3798db1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, pipeline\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "979542a7-50cc-4057-a39a-75f96fe5e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"db-dtypes\", \"google-cloud-bigquery\", \"pyarrow\"]\n",
    ")\n",
    "def stage(bq_projectid: str, bq_dataset: str, bq_table: str, output_dataset: OutputPath('staged_bq_table')):\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    ##authenticate \n",
    "    auth_credentials, auth_project = google.auth.default()\n",
    "    print(\"Project: \"+auth_project)\n",
    "    client = bigquery.Client(project=bq_projectid, credentials = auth_credentials)\n",
    "    \n",
    "    \n",
    "    query = f\"SELECT * FROM {bq_projectid}.{bq_dataset}.{bq_table}\"\n",
    "    print(query)\n",
    "    \n",
    "    ## fetch query results as dataframe\n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head()) \n",
    "    \n",
    "    ## export resultset into csv file om GCS\n",
    "    dataframe.to_csv(output_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc34ab5-50ce-41e2-8091-aac59f674af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"wf-kubeflow-bq2gcs\")\n",
    "def pipeline(\n",
    "    in_bq_projectid: str = 'defaultprojectid',\n",
    "    in_bq_dataset: str = 'xxxx',\n",
    "    in_bq_table: str = 'yyyy'\n",
    "):\n",
    "    stagingTask = stage(bq_projectid = in_bq_projectid,\n",
    "                                   bq_dataset   = in_bq_dataset, \n",
    "                                   bq_table     = in_bq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b603a16c-4d21-4f10-a05f-44557fc86c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_yaml_filename=\"dag_kubeflow_bq2gcs.json\"   ##The output path dag_kubeflow_bq2gcs.yaml should ends with \".json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "400ee85b-7fec-4745-ac9b-e7896873abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline,\n",
    "   package_path=dag_yaml_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51ba26b2-8b71-44c4-95d1-eecaae84fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PARAMETERS = {\n",
    "    \"in_bq_projectid\":  BIGQUERY_PROJECT_ID, \n",
    "    \"in_bq_dataset\":    BIGQUERY_DATASET,\n",
    "    \"in_bq_table\":      BIGQUERY_TABLE\n",
    "}\n",
    "\n",
    "LABELS = {}\n",
    "ENABLE_CACHING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d853da8d-3872-4adc-b7fa-808160147875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/wf-kubeflow-bq2gcs-20230214142528?project=339239659794\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"kfp_pipeline_bq2gcs\",\n",
    "                             template_path = dag_yaml_filename,\n",
    "                             ##pipeline_root = PIPELINE_ROOT,\n",
    "                             parameter_values = PIPELINE_PARAMETERS, ## Make sure PIPELINE_PARAMETERS collection does not include parameters that are unknown to pipeline\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             labels = LABELS,\n",
    "                             project = VERTEXAI_PROJECT_ID,\n",
    "                             location = VERTEXAI_REGION)\n",
    "\n",
    "job.run(service_account=\"339239659794-compute@developer.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b329da63-632d-48aa-a982-716d0c39ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"db-dtypes\",\"scikit-learn\", \"google-cloud-bigquery\", \"pyarrow\"]\n",
    ")\n",
    "def preprocess(staged_dataset: InputPath('staged_bq_table'), \n",
    "               staged_training_dataset: OutputPath('staged_training_dataset'), \n",
    "               staged_validation_dataset: OutputPath('staged_validation_dataset'), \n",
    "               staged_test_dataset: OutputPath('staged_test_dataset')):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    dataset = pd.read_csv(staged_bq_table, index_col=0)\n",
    "    _excluded_columns = [\"synerise_client_id\"]\n",
    "    _target_column=\"y_if_trans\"\n",
    "    _weight_column=\"weight\"\n",
    "    \n",
    "    ## drop columns that are not needed\n",
    "    dataset.drop(_excluded_columns, axis=1)\n",
    "    \n",
    "    dataset = dataset.loc[:, dataset.columns != _target_column]\n",
    "    \n",
    "    X = dataset.loc[:, dataset.columns != _target_column]\n",
    "    ## Feature engineering if any, e.g\n",
    "    ## from sklearn.preprocessing import MinMaxScaler\n",
    "    ## scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    ## scaler.fit(X)\n",
    "    \n",
    "    Y = dataset[_target_column]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=101)\n",
    "    \n",
    "    training_dataset = pd.concat([X_train,Y_train], axis = 1)\n",
    "    validation_dataset = pd.concat([X_val,Y_val], axis = 1)\n",
    "    test_dataset = pd.concat([X_test,Y_test], axis = 1)\n",
    "\n",
    "    training_dataset.to_csv(staged_training_dataset)\n",
    "    validation_dataset.to_csv(staged_validation_dataset)\n",
    "    test_dataset.to_csv(staged_test_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f21623-d218-4878-ada5-e44c44bd016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"db-dtypes\",\"scikit-learn\", \"pyarrow\"]\n",
    ")\n",
    "def train(staged_training_dataset: InputPath('staged_training_dataset'), \n",
    "          staged_validation_dataset: InputPath('staged_validation_dataset'), \n",
    "          staged_test_dataset: InputPath('staged_test_dataset'),\n",
    "          in_vertexai_experiment_name:str, \n",
    "          in_vertexai_region: str, \n",
    "          in_vertexai_projectid: str, \n",
    "          output_model: Output[Model]\n",
    "         ):\n",
    "    \n",
    "    import sklearn.metrics as metrics\n",
    "    from google.cloud import aiplatform\n",
    "    from datetime import datetime\n",
    "    import tensorflow as tf\n",
    "    import keras_tuner \n",
    "    \n",
    "    _METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    ## function to build model\n",
    "    def build_model(hptune):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation = \"relu\"))\n",
    "        model.add(\n",
    "           Dense(\n",
    "              # Define the hyperparameter\n",
    "              units=hptune.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "              activation=hptune.Choice(\"activation\",[\"relu\",\"tanh\"]),\n",
    "                )\n",
    "        )\n",
    "        if hptune.Boolean(\"dropout\"):\n",
    "           model.add(Dropout(rate=0.25))\n",
    "    \n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        learning_rate = hptune.Float(\"lr\",min_value = 1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "            metrics=_METRICS,\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    training_dataset = pd.read_csv(staged_training_dataset)\n",
    "    validation_dataset = pd.read_csv(staged_validation_dataset)\n",
    "    test_dataset = pd.read_csv(staged_test_dataset)\n",
    "    \n",
    "    ##Create a Keras Hyperband Hyperparameter tuner with an accuracy objective\n",
    "    tuner =  keras_tuner.Hyperband(\n",
    "       hypermodel=build_model,\n",
    "       objective=\"val_accuracy\",\n",
    "       max_epochs=50,\n",
    "       factor=3,\n",
    "       hyperband_iterations=1,\n",
    "       seed=None,\n",
    "       hyperparameters=None,\n",
    "       tune_new_entries=True,\n",
    "       allow_new_entries=True\n",
    "    )\n",
    "    \n",
    "    stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val) , callbacks=[stop_early]) ## you can also use validation_split=0.2 if you do not have validation data\n",
    "    \n",
    "    # Get the optimal hyperparameters for the model as determined from the search\n",
    "    best_hyperparameters=tuner.get_best_hyperparameters()[0]\n",
    "    hypermodel = tuner.hypermodel.build(best_hyperparameters)\n",
    "    history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val))\n",
    "    \n",
    "    ## register hyperparameters and metrics as Vertex AI experiment run\n",
    "    aiplatform.init(\n",
    "       project=in_vertexai_projectid,\n",
    "       location=in_vertexai_region,\n",
    "       experiment=in_vertexai_experiment_name\n",
    "    )\n",
    "    \n",
    "    run_id = f\"run-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    aiplatform.start_run(run_id)\n",
    "    \n",
    "    training_params = {\n",
    "        'training_dataset': staged_training_dataset,\n",
    "        'validation_dataset': staged_validation_dataset,\n",
    "        'test_dataset': staged_test_dataset,\n",
    "        'model_type': 'nn',\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    \n",
    "    training_metrics = {\n",
    "        'model_accuracy': metrics.accuracy_score(Y_test, predicted),\n",
    "        'model_precision': metrics.precision_score(Y_test, predicted, average='macro'),\n",
    "        'model_recall': metrics.recall_score(Y_test, predicted, average='macro'),\n",
    "        'model_logloss': metrics.log_loss(Y_test, predicted),\n",
    "        'model_auc_roc': metrics.roc_auc_score(Y_test, predicted)\n",
    "    }\n",
    "    \n",
    "    aiplatform.log_params(training_params)\n",
    "    aiplatform.log_metrics(training_metrics)\n",
    "    \n",
    "    model_path=os.path.split(output_model.path)\n",
    "    hypermodel.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c7a6a-9e1a-485b-b6ac-5b5a71da0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"db-dtypes\",\"scikit-learn\", \"google-cloud-bigquery\", \"pyarrow\"]\n",
    ")\n",
    "def gate(staged_training_dataset: InputPath('staged_training_dataset')): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391388e3-8c2f-4fc5-8200-84264a2e7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "   packages_to_install=[\"pandas\", \"google-cloud-aiplatform\", \"google-cloud-storage\"]\n",
    ")\n",
    "def deploy():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4112256-fb1e-4239-b52e-72bec398ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import Condition\n",
    "\n",
    "@pipeline(name=\"wf-mlops_pipeline\")\n",
    "def pipeline(\n",
    "    wf_bq_projectid: str = 'defaultprojectid',\n",
    "    wf_bq_dataset: str = 'xxxx',\n",
    "    wf_bq_table: str = 'yyyy',\n",
    "    \n",
    "    wf_vertexai_project_id: str = 'defaultprojectid'\n",
    "):\n",
    "    stagingTask = stage(bq_projectid = wf_bq_projectid,\n",
    "                                   bq_dataset   = wf_bq_dataset, \n",
    "                                   bq_table     = wf_bq_table)\n",
    "    \n",
    "    preprocessingTask = preprocess(stagingTask.output)\n",
    "    \n",
    "    trainingTask = train(staged_training_dataset = preprocessingTask.outputs['staged_training_dataset'],\n",
    "                         staged_validation_dataset = preprocessingTask.outputs['staged_validation_dataset'],\n",
    "                         staged_test_dataset = preprocessingTask.outputs['staged_test_dataset'],\n",
    "                         in_vertexai_experiment_name = wf_vertexai_experiment, \n",
    "                         in_vertexai_region = wf_vertexai_region, \n",
    "                         in_vertexai_projectid = wf_vertexai_project_id, \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b51be5-b8de-4e03-912d-93a3d55fc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline,\n",
    "   package_path=dag_yaml_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141eb83-3de8-45c9-bdd9-090ec5a466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PARAMETERS = {\n",
    "    \"in_bq_projectid\":  BIGQUERY_PROJECT_ID, \n",
    "    \"in_bq_dataset\":    BIGQUERY_DATASET,\n",
    "    \"in_bq_table\":      BIGQUERY_TABLE\n",
    "}\n",
    "\n",
    "LABELS = {}\n",
    "ENABLE_CACHING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8813e-2da6-48e2-8f86-debe08c447fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"kfp_pipeline_bq2gcs\",\n",
    "                             template_path = dag_yaml_filename,\n",
    "                             ##pipeline_root = PIPELINE_ROOT,\n",
    "                             parameter_values = PIPELINE_PARAMETERS, ## Make sure PIPELINE_PARAMETERS collection does not include parameters that are unknown to pipeline\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             labels = LABELS,\n",
    "                             project = VERTEXAI_PROJECT_ID,\n",
    "                             location = VERTEXAI_REGION)\n",
    "\n",
    "job.run(service_account=\"339239659794-compute@developer.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

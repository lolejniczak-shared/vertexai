apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: wf-kubeflow-bq2gcs-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.14, pipelines.kubeflow.org/pipeline_compilation_time: '2023-02-14T14:06:18.861086',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "defaultprojectid",
      "name": "in_bq_projectid", "optional": true, "type": "String"}, {"default":
      "xxxx", "name": "in_bq_dataset", "optional": true, "type": "String"}, {"default":
      "yyyy", "name": "in_bq_table", "optional": true, "type": "String"}, {"default":
      "gs://gcp-demo-ccc-vertexai/pipeline_root", "name": "pipeline-root"}], "name":
      "wf-kubeflow-bq2gcs"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.14}
spec:
  entrypoint: wf-kubeflow-bq2gcs
  templates:
  - name: preprocess
    container:
      args: [--executor_input, '{{$}}', --function_to_execute, preprocess]
      command:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp-pipeline-spec==0.1.16' 'pandas' 'google-cloud-bigquery' 'pyarrow' 'kfp==1.8.14' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef preprocess(bq_projectid: str, bq_dataset: str, bq_table:\
        \ str, output_dataset: Output[Dataset]):\n    from google.cloud import bigquery\n\
        \    import google.auth\n\n    ##authenticate \n    auth_credentials, auth_project\
        \ = google.auth.default()\n    print(\"Project: \"+auth_project)\n    client\
        \ = bigquery.Client(project=bqprojectid, credentials = auth_credentials)\n\
        \n\n    query = f\"SELECT * FROM {bq_projectid}.{bq_dataset}.{bq_table}\"\n\
        \    print(query)\n\n    ## fetch query results as dataframe\n    dataframe\
        \ = client.query(query).to_dataframe()\n    print(dataframe.head()) \n\n \
        \   ## export resultset into csv file om GCS\n    dataframe.to_csv(output_dataset.path)\n\
        \n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: in_bq_dataset, path: /tmp/inputs/bq_dataset/data}
      - {name: in_bq_projectid, path: /tmp/inputs/bq_projectid/data}
      - {name: in_bq_table, path: /tmp/inputs/bq_table/data}
    outputs:
      artifacts:
      - {name: preprocess-output_dataset, path: /tmp/outputs/output_dataset/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.14
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--executor_input", {"executorInput": null}, "--function_to_execute",
          "preprocess"], "command": ["sh", "-c", "\nif ! [ -x \"$(command -v pip)\"
          ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get
          install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet     --no-warn-script-location ''kfp-pipeline-spec==0.1.16''
          ''pandas'' ''google-cloud-bigquery'' ''pyarrow'' ''kfp==1.8.14'' && \"$0\"
          \"$@\"\n", "sh", "-ec", "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\"
          > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
          "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing
          import *\n\ndef preprocess(bq_projectid: str, bq_dataset: str, bq_table:
          str, output_dataset: Output[Dataset]):\n    from google.cloud import bigquery\n    import
          google.auth\n\n    ##authenticate \n    auth_credentials, auth_project =
          google.auth.default()\n    print(\"Project: \"+auth_project)\n    client
          = bigquery.Client(project=bqprojectid, credentials = auth_credentials)\n\n\n    query
          = f\"SELECT * FROM {bq_projectid}.{bq_dataset}.{bq_table}\"\n    print(query)\n\n    ##
          fetch query results as dataframe\n    dataframe = client.query(query).to_dataframe()\n    print(dataframe.head())
          \n\n    ## export resultset into csv file om GCS\n    dataframe.to_csv(output_dataset.path)\n\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "bq_projectid", "type": "String"},
          {"name": "bq_dataset", "type": "String"}, {"name": "bq_table", "type": "String"}],
          "name": "Preprocess", "outputs": [{"name": "output_dataset", "type": "Dataset"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: wf-kubeflow-bq2gcs
    inputs:
      artifacts:
      - {name: in_bq_dataset}
      - {name: in_bq_projectid}
      - {name: in_bq_table}
    dag:
      tasks:
      - name: preprocess
        template: preprocess
        arguments:
          artifacts:
          - {name: in_bq_dataset, from: '{{inputs.artifacts.in_bq_dataset}}'}
          - {name: in_bq_projectid, from: '{{inputs.artifacts.in_bq_projectid}}'}
          - {name: in_bq_table, from: '{{inputs.artifacts.in_bq_table}}'}
  arguments:
    parameters:
    - {name: in_bq_projectid, value: defaultprojectid}
    - {name: in_bq_dataset, value: xxxx}
    - {name: in_bq_table, value: yyyy}
    - {name: pipeline-root, value: 'gs://gcp-demo-ccc-vertexai/pipeline_root'}
    artifacts:
    - name: in_bq_projectid
      raw: {data: '{{workflow.parameters.in_bq_projectid}}'}
    - name: in_bq_dataset
      raw: {data: '{{workflow.parameters.in_bq_dataset}}'}
    - name: in_bq_table
      raw: {data: '{{workflow.parameters.in_bq_table}}'}
  serviceAccountName: pipeline-runner

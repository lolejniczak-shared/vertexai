{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b013d79-ffb3-473d-a873-ad0bef699a2a",
   "metadata": {},
   "source": [
    "### What we will learn\n",
    "\n",
    "- We will build custom Kubeflow pipeline for classfication\n",
    "- We will show how to use Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14bd11f9-6e75-4ee8-82fd-3141cd4a5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a2454c7-5fc4-43fa-b2f4-46ca3a7c4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.14\n"
     ]
    }
   ],
   "source": [
    "print(kfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a01739a-0aab-4a9d-ae9e-670f6d4df2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d90a7f6-aba3-4dde-9ad6-fbcf31c23e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGQUERY_PROJECT_ID:  datafusionsbox\n",
      "BIGQUERY_DATASET:  dataset4ccc\n",
      "BIGQUERY_DATASET_REGION:  us\n",
      "BIGQUERY_TABLE:  df_for_model_ccc_with_weights\n",
      "VERTEXAI_PROJECT_ID:  datafusionsbox\n",
      "VERTEXAI_REGION:  us-central1\n",
      "BUCKET_NAME:  gcp-demo-ccc-vertexai\n",
      "BUCKET_URI:  gcp-demo-ccc-vertexai\n",
      "BUCKET_REGION:  us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BIGQUERY_PROJECT_ID = os.environ.get('BIGQUERY_PROJECT_ID')\n",
    "BIGQUERY_DATASET = os.environ.get('BIGQUERY_DATASET')\n",
    "BIGQUERY_DATASET_REGION = os.environ.get('BIGQUERY_DATASET_REGION')\n",
    "BIGQUERY_TABLE = os.environ.get('BIGQUERY_TABLE')\n",
    "\n",
    "VERTEXAI_PROJECT_ID = os.environ.get('VERTEXAI_PROJECT_ID')\n",
    "VERTEXAI_REGION = os.environ.get('VERTEXAI_REGION')\n",
    "\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
    "BUCKET_URI = os.environ.get('BUCKET_URI')\n",
    "BUCKET_REGION = os.environ.get('BUCKET_REGION')\n",
    "\n",
    "PREFIX = os.environ.get('PREFIX')\n",
    "\n",
    "print(\"BIGQUERY_PROJECT_ID: \",BIGQUERY_PROJECT_ID)\n",
    "print(\"BIGQUERY_DATASET: \",BIGQUERY_DATASET)\n",
    "print(\"BIGQUERY_DATASET_REGION: \",BIGQUERY_DATASET_REGION)\n",
    "print(\"BIGQUERY_TABLE: \",BIGQUERY_TABLE)\n",
    "\n",
    "print(\"VERTEXAI_PROJECT_ID: \",VERTEXAI_PROJECT_ID)\n",
    "print(\"VERTEXAI_REGION: \",VERTEXAI_REGION)\n",
    "\n",
    "print(\"BUCKET_NAME: \",BUCKET_NAME)\n",
    "print(\"BUCKET_URI: \",BUCKET_NAME)\n",
    "print(\"BUCKET_REGION: \",VERTEXAI_REGION)\n",
    "\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root'.format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc26e95-4604-4be3-8d6c-ac3798db1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, pipeline\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "979542a7-50cc-4057-a39a-75f96fe5e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"db-dtypes\", \"google-cloud-bigquery\", \"pyarrow\"]\n",
    ")\n",
    "def stage(bq_projectid: str, bq_dataset: str, bq_table: str, output_dataset: OutputPath('staged_bq_table')):\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    ##authenticate \n",
    "    auth_credentials, auth_project = google.auth.default()\n",
    "    print(\"Project: \"+auth_project)\n",
    "    client = bigquery.Client(project=bq_projectid, credentials = auth_credentials)\n",
    "    \n",
    "    \n",
    "    query = f\"SELECT * FROM {bq_projectid}.{bq_dataset}.{bq_table}\"\n",
    "    print(query)\n",
    "    \n",
    "    ## fetch query results as dataframe\n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head()) \n",
    "    \n",
    "    ## export resultset into csv file om GCS\n",
    "    dataframe.to_csv(output_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc34ab5-50ce-41e2-8091-aac59f674af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"wf-kubeflow-bq2gcs\")\n",
    "def pipeline(\n",
    "    in_bq_projectid: str = 'defaultprojectid',\n",
    "    in_bq_dataset: str = 'xxxx',\n",
    "    in_bq_table: str = 'yyyy'\n",
    "):\n",
    "    stagingTask = stage(bq_projectid = in_bq_projectid,\n",
    "                                   bq_dataset   = in_bq_dataset, \n",
    "                                   bq_table     = in_bq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b603a16c-4d21-4f10-a05f-44557fc86c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_json_filename = \"dag_ccc_ex3b_bq2gcs.json\"   ##The output path dag_kubeflow_bq2gcs.yaml should ends with \".json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "400ee85b-7fec-4745-ac9b-e7896873abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline,\n",
    "   package_path=dag_yaml_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51ba26b2-8b71-44c4-95d1-eecaae84fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PARAMETERS = {\n",
    "    \"in_bq_projectid\":  BIGQUERY_PROJECT_ID, \n",
    "    \"in_bq_dataset\":    BIGQUERY_DATASET,\n",
    "    \"in_bq_table\":      BIGQUERY_TABLE\n",
    "}\n",
    "\n",
    "LABELS = {}\n",
    "ENABLE_CACHING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d853da8d-3872-4adc-b7fa-808160147875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/wf-kubeflow-bq2gcs-20230214142528?project=339239659794\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-bq2gcs-20230214142528\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"kfp_pipeline_bq2gcs\",\n",
    "                             template_path = dag_json_filename,\n",
    "                             ##pipeline_root = PIPELINE_ROOT,\n",
    "                             parameter_values = PIPELINE_PARAMETERS, ## Make sure PIPELINE_PARAMETERS collection does not include parameters that are unknown to pipeline\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             labels = LABELS,\n",
    "                             project = VERTEXAI_PROJECT_ID,\n",
    "                             location = VERTEXAI_REGION)\n",
    "\n",
    "job.run(service_account=\"339239659794-compute@developer.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b329da63-632d-48aa-a982-716d0c39ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"fsspec\",\"gcsfs\",\"scikit-learn\"]\n",
    ")\n",
    "def preprocess(gcs_pipeline_root: str,\n",
    "               app_prefix: str,\n",
    "               user_id_column: str,\n",
    "               target_column: str,\n",
    "               weight_column: str,\n",
    "               excluded_columns: list,\n",
    "               staged_bq_table: InputPath('staged_bq_table'), \n",
    "               staged_training_dataset: OutputPath('staged_training_dataset'), \n",
    "               staged_validation_dataset: OutputPath('staged_validation_dataset'), \n",
    "               staged_test_dataset: OutputPath('staged_test_dataset')):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    dataset = pd.read_csv(staged_bq_table, index_col=0)\n",
    "    \n",
    "    \n",
    "    ## drop excluded columns\n",
    "    ndataset = dataset.drop(excluded_columns, axis =1)\n",
    "    \n",
    "    X = ndataset.loc[:, ndataset.columns != target_column]\n",
    "    Y = ndataset.loc[:, ndataset.columns == target_column]\n",
    "    ## Feature engineering if any, e.g\n",
    "    ## from sklearn.preprocessing import MinMaxScaler\n",
    "    ## scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    ## scaler.fit(X)\n",
    "    \n",
    "    ## Split dataset into training, validation and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=101)\n",
    "    \n",
    "    training_dataset = pd.concat([X_train,Y_train], axis = 1)\n",
    "    validation_dataset = pd.concat([X_val,Y_val], axis = 1)\n",
    "    test_dataset = pd.concat([X_test,Y_test], axis = 1)\n",
    "\n",
    "    ## Stage training, validation and testing datasets to GCS\n",
    "    training_dataset.to_csv(staged_training_dataset, index = False)\n",
    "    validation_dataset.to_csv(staged_validation_dataset,index = False)\n",
    "    test_dataset.to_csv(staged_test_dataset, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "086cd18c-2b2b-4836-9b95-e7e8ab1ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PARAMETERS = {\n",
    "    \"in_bq_projectid\":  BIGQUERY_PROJECT_ID, \n",
    "    \"in_bq_dataset\":    BIGQUERY_DATASET,\n",
    "    \"in_bq_table\":      BIGQUERY_TABLE,\n",
    "    \"in_pipeline_root\": PIPELINE_ROOT,    ### THIS IS NEW HERE\n",
    "    \"in_app_prefix\":    PREFIX            ### THIS IS NEW HERE\n",
    "}\n",
    "\n",
    "LABELS = {}\n",
    "ENABLE_CACHING=True\n",
    "\n",
    "dag_json_filename = \"dag_ccc_ex3b_mlops.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "969e5e78-dd43-4791-9332-5eb6deb5a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, pipeline\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29f7b502-f7c9-4362-b76f-912c9c97fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"wf-kubeflow-mlops\")  ##WE CHANGE THE NAME\n",
    "def pipeline(\n",
    "    in_bq_projectid: str = 'defaultprojectid',\n",
    "    in_bq_dataset: str = 'xxxx',\n",
    "    in_bq_table: str = 'yyyy',\n",
    "    in_pipeline_root: str = '',    ### NEW PIPELINE PARAMETERS\n",
    "    in_app_prefix: str = 'demo'    ### NEW PIPELINE PARAMETERS\n",
    "):\n",
    "    stagingTask = stage(bq_projectid = in_bq_projectid,\n",
    "                                   bq_dataset   = in_bq_dataset, \n",
    "                                   bq_table     = in_bq_table)\n",
    "    \n",
    "    _user_id_column=\"synerise_client_id\"\n",
    "    _target_column=\"y_if_trans\"\n",
    "    _weight_column=\"weight\"\n",
    "    _excluded_columns = [_user_id_column, _weight_column]\n",
    "    \n",
    "    preprocessTask = preprocess(gcs_pipeline_root = in_pipeline_root,\n",
    "                                app_prefix = in_app_prefix, \n",
    "                                user_id_column = _user_id_column,\n",
    "                                target_column=_target_column,\n",
    "                                wight_column=_weight_column,\n",
    "                                excluded_columns = _excluded_columns,\n",
    "                                staged_bq_table = stagingTask.outputs[\"output_dataset\"]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e3de1ed-d2f0-4e90-9f60-81db1cfa87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline,\n",
    "   package_path=dag_json_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb8d4233-42b0-459f-89c7-bac61d66ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/wf-kubeflow-mlops-20230221090954?project=339239659794\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-mlops-20230221090954\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"kfp_pipeline_mlops\",\n",
    "                             template_path = dag_yaml_filename,\n",
    "                             ##pipeline_root = PIPELINE_ROOT,\n",
    "                             parameter_values = PIPELINE_PARAMETERS, ## Make sure PIPELINE_PARAMETERS collection does not include parameters that are unknown to pipeline\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             labels = LABELS,\n",
    "                             project = VERTEXAI_PROJECT_ID,\n",
    "                             location = VERTEXAI_REGION)\n",
    "\n",
    "job.run(service_account=\"339239659794-compute@developer.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf448f9-fb04-41af-afe5-98145cad01f6",
   "metadata": {},
   "source": [
    "### It goes like that .... \n",
    "So lets add stapes for training, decision gate and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "07f21623-d218-4878-ada5-e44c44bd016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"fsspec\",\"gcsfs\",\"scikit-learn\", \"google-cloud-aiplatform\", \"keras_tuner\"],\n",
    "    base_image = \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    ")\n",
    "def train(staged_training_dataset: InputPath('staged_training_dataset'), \n",
    "          staged_validation_dataset: InputPath('staged_validation_dataset'), \n",
    "          staged_test_dataset: InputPath('staged_test_dataset'),\n",
    "          vertexai_experiment_name:str, \n",
    "          vertexai_region: str, \n",
    "          vertexai_projectid: str,\n",
    "          gcs_pipeline_root: str,\n",
    "          app_prefix: str, \n",
    "          user_id_column: str,\n",
    "          target_column: str,\n",
    "          weight_column: str,\n",
    "          excluded_columns: list,\n",
    "          output_model: Output[Model]\n",
    "         ):\n",
    "    \n",
    "     import tensorflow as tf\n",
    "     import keras_tuner\n",
    "     from google.cloud import aiplatform\n",
    "     from datetime import datetime\n",
    "     import pandas as pd\n",
    "     \n",
    "    \n",
    "     _METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "     ]\n",
    "    \n",
    "     ## function to build model\n",
    "     def build_model(hptune):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(units=32, activation = \"relu\"))\n",
    "        model.add(\n",
    "           tf.keras.layers.Dense(\n",
    "              # Define the hyperparameter\n",
    "              units=32, ##hptune.Int(\"units\", min_value=32, max_value=96, step=32),\n",
    "              activation=\"relu\" ##hptune.Choice(\"activation\",[\"relu\",\"tanh\"]),\n",
    "                )\n",
    "        )\n",
    "        if hptune.Boolean(\"dropout\"):\n",
    "           model.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "    \n",
    "        model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "        learning_rate = 1e-4 ##hptune.Float(\"lr\",min_value = 1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "            metrics=_METRICS,\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "     training_dataset = pd.read_csv(staged_training_dataset)\n",
    "     validation_dataset = pd.read_csv(staged_validation_dataset)\n",
    "     test_dataset = pd.read_csv(staged_test_dataset)\n",
    "        \n",
    "        \n",
    "     feature_columns = [column for column in training_dataset.columns if  column != target_column]\n",
    "     target_columns = [target_column]\n",
    "\n",
    "     x_train = training_dataset[feature_columns]\n",
    "     y_train = training_dataset[target_columns]\n",
    "        \n",
    "     x_val = validation_dataset[feature_columns]\n",
    "     y_val = validation_dataset[target_columns]\n",
    "\n",
    "     x_test = test_dataset[feature_columns]\n",
    "     y_test = test_dataset[target_columns]\n",
    "        \n",
    "     trials_dir=f\"{app_prefix}_trials\"\n",
    "     ##Create a Keras Hyperband Hyperparameter tuner with an accuracy objective\n",
    "     tuner =  keras_tuner.Hyperband(\n",
    "       hypermodel=build_model,\n",
    "       objective=keras_tuner.Objective(\"precision\", direction=\"max\"),\n",
    "       max_epochs=2,\n",
    "       factor=3,\n",
    "       hyperband_iterations=1,\n",
    "       seed=None,\n",
    "       hyperparameters=None,\n",
    "       tune_new_entries=True,\n",
    "       allow_new_entries=True,\n",
    "       directory=trials_dir\n",
    "     )\n",
    "    \n",
    "     weight_for_0 = 0.5 ##(1 / neg) * (total / 2.0)\n",
    "     weight_for_1 = 20  ##(1 / pos) * (total / 2.0)\n",
    "\n",
    "     class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "     stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "     output = tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val) , callbacks=[stop_early], class_weight=class_weights)\n",
    "        \n",
    "     # Get the optimal hyperparameters for the model as determined from the search\n",
    "     best_hyperparameters=tuner.get_best_hyperparameters()[0]\n",
    "     hypermodel = tuner.hypermodel.build(best_hyperparameters)\n",
    "     history = hypermodel.fit(x_train, y_train, epochs=2, validation_data=(x_val, y_val))\n",
    "        \n",
    "     results = hypermodel.evaluate(x_test, y_test)\n",
    "    \n",
    "     #### SAVE MODEL\n",
    "     print(output_model.path)\n",
    "     model_path = output_model.path\n",
    "     hypermodel.save(model_path)\n",
    "        \n",
    "        \n",
    "     aiplatform.init(\n",
    "       project=vertexai_projectid,\n",
    "       location=vertexai_region,\n",
    "       experiment=vertexai_experiment_name\n",
    "     )\n",
    "    \n",
    "     run_id = f\"run-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "     run = aiplatform.start_run(run_id)\n",
    "    \n",
    "     training_params = {\n",
    "        'training_dataset': staged_training_dataset,\n",
    "        'validation_dataset': staged_validation_dataset,\n",
    "        'test_dataset': staged_test_dataset,\n",
    "        'model_type': 'nn',\n",
    "        'model_path': model_path,\n",
    "        'trainedby': app_prefix, \n",
    "        ##'hp_units': best_hyperparameters.get('units'),\n",
    "        ##'hp_activation': best_hyperparameters.get('activation'),\n",
    "        'hp_dropout': best_hyperparameters.get('dropout'),\n",
    "        ##'hp_lr': best_hyperparameters.get('lr'),\n",
    "     }\n",
    "    \n",
    "     training_metrics = {\n",
    "        'model_loss': results[0],\n",
    "        'model_accuracy': results[5],\n",
    "        'model_precision': results[6],\n",
    "        'model_recall': results[7],\n",
    "        'model_auc': results[8],\n",
    "        'model_prc': results[9],\n",
    "        'model_tp': results[1],\n",
    "        'model_fp': results[2],\n",
    "        'model_tn': results[3],\n",
    "        'model_fn': results[4]\n",
    "     }\n",
    "    \n",
    "     run.log_params(training_params)\n",
    "     run.log_metrics(training_metrics)\n",
    "\n",
    "\n",
    "     classification_metrics = run.log_classification_metrics(\n",
    "       display_name='classification metrics',\n",
    "       labels=['Positive', 'Negative'],\n",
    "       matrix=[[results[1], results[2]], [results[4], results[3]]],\n",
    "       fpr=[],\n",
    "       tpr=[],\n",
    "       threshold=[],\n",
    "     )\n",
    "\n",
    "     run.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a398c8-3ab9-4613-a6db-e269d823dc28",
   "metadata": {},
   "source": [
    "### Upload model to Vertex AI Model registry and deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "580d172e-4d90-47c2-b5c1-c5f7e65d080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\",\"fsspec\",\"gcsfs\",\"scikit-learn\", \"google-cloud-aiplatform\"]\n",
    ")\n",
    "def simpledeploy(hmodel: Input[Model],\n",
    "          vertexai_experiment_name:str, \n",
    "          vertexai_region: str, \n",
    "          vertexai_projectid: str,\n",
    "          app_prefix: str,\n",
    "          serving_machine_type: str,\n",
    "          serving_min_replica: int,\n",
    "          serving_max_replica: int           \n",
    "         ):\n",
    "    \n",
    "     from google.cloud import aiplatform \n",
    "        \n",
    "     aiplatform.init(\n",
    "       project=vertexai_projectid,\n",
    "       location=vertexai_region\n",
    "     )\n",
    "     \n",
    "     ## auxiliary variables\n",
    "     model_name = f\"{app_prefix}_model_exc3b_mlops\"\n",
    "     endpoint_name = f\"{app_prefix}_endpoint_exc3b_mlops\"\n",
    "     model_path = hmodel.path\n",
    "    \n",
    "     ##check if model is already registered in Vertex AI Model Registry\n",
    "     model_filter_str='labels.experiment_name=\"'+vertexai_experiment_name+'\"'\n",
    "     print(\"Model filter string: \"+model_filter_str)\n",
    "    \n",
    "     models = aiplatform.Model.list(\n",
    "        filter=model_filter_str\n",
    "     )\n",
    "    \n",
    "     model_labels = {\n",
    "          \"experiment_name\": vertexai_experiment_name\n",
    "     }\n",
    "        \n",
    "     if len(models)>0:\n",
    "        model_exists = True\n",
    "        model = models[0]\n",
    "    \n",
    "        vertexai_model = aiplatform.Model.upload_tensorflow_saved_model(\n",
    "          display_name = model_name,\n",
    "          parent_model = model.resource_name,\n",
    "          saved_model_dir = model_path,\n",
    "          labels = model_labels,\n",
    "          is_default_version = True\n",
    "        )\n",
    "     else: \n",
    "        vertexai_model = aiplatform.Model.upload_tensorflow_saved_model(\n",
    "          display_name = model_name,\n",
    "          saved_model_dir = model_path,\n",
    "          labels = model_labels,\n",
    "          is_default_version = True\n",
    "        )\n",
    "        \n",
    "        \n",
    "     ##same story for endpoint - check if exists - if not create it and then deploy new model to it. \n",
    "     endpoint_filter_str='labels.experiment_name=\"'+vertexai_experiment_name+'\"'\n",
    "     endpoints = aiplatform.Endpoint.list(\n",
    "       filter=endpoint_filter_str,\n",
    "     )\n",
    "    \n",
    "     endpoint_labels = {\n",
    "         \"experiment_name\": vertexai_experiment_name\n",
    "     }\n",
    "        \n",
    "     if len(endpoints)>0:\n",
    "       endpoint = endpoints[0]\n",
    "       deployed_models = endpoint.list_models()\n",
    "       for dmodel in deployed_models:\n",
    "           print(dmodel.display_name)\n",
    "     else: \n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "          display_name = endpoint_name,\n",
    "          labels = endpoint_labels   \n",
    "        )\n",
    "    \n",
    "     #### Deploy model to endpoint\n",
    "     endpoint.deploy(\n",
    "       model = vertexai_model,\n",
    "       traffic_percentage = 100,\n",
    "       machine_type=serving_machine_type,\n",
    "       min_replica_count=serving_min_replica,\n",
    "       max_replica_count=serving_max_replica\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8e9e65b8-7b91-460c-a882-6e68894a6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_PARAMETERS = {\n",
    "    \"in_bq_projectid\":  BIGQUERY_PROJECT_ID, \n",
    "    \"in_bq_dataset\":    BIGQUERY_DATASET,\n",
    "    \"in_bq_table\":      BIGQUERY_TABLE,\n",
    "    \"in_pipeline_root\": PIPELINE_ROOT,    ### THIS IS NEW HERE\n",
    "    \"in_app_prefix\":    PREFIX,           ### THIS IS NEW HERE\n",
    "    \"in_vertexai_experiment_name\": f\"{PREFIX}-experiments\", ### NEW PIPELINE PARAMETERS\n",
    "    \"in_vertexai_region\":    VERTEXAI_REGION,          ### NEW PIPELINE PARAMETERS\n",
    "    \"in_vertexai_projectid\": VERTEXAI_PROJECT_ID,       ### NEW PIPELINE PARAMETERS\n",
    "    \"in_serving_machine_type\": \"n1-standard-4\",     ### NEW PIPELINE PARAMETERS\n",
    "    \"in_serving_min_replica\": 1,      ### NEW PIPELINE PARAMETERS\n",
    "    \"in_serving_max_replica\": 2      ### NEW PIPELINE PARAMETERS\n",
    "}\n",
    "\n",
    "LABELS = {}\n",
    "ENABLE_CACHING=True\n",
    "\n",
    "dag_json_filename = \"dag_ccc_ex3b_e2e_mlops.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "edf6a89d-d973-49b5-8e52-bfc60b72f7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_bq_projectid': 'datafusionsbox',\n",
       " 'in_bq_dataset': 'dataset4ccc',\n",
       " 'in_bq_table': 'df_for_model_ccc_with_weights',\n",
       " 'in_pipeline_root': 'gs://gcp-demo-ccc-vertexai/pipeline_root',\n",
       " 'in_app_prefix': 'ccc',\n",
       " 'in_vertexai_experiment_name': 'ccc-experiments',\n",
       " 'in_vertexai_region': 'us-central1',\n",
       " 'in_vertexai_projectid': 'datafusionsbox',\n",
       " 'in_serving_machine_type': 'n1-standard-4',\n",
       " 'in_serving_min_replica': 1,\n",
       " 'in_serving_max_replica': 2}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fbae90ea-96e7-4c73-b6b0-889db7ff162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, pipeline\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "81ba9682-924e-4be2-9c82-8c0976f0103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"wf-kubeflow-e2e-mlops\")  ##WE CHANGE THE NAME e2e\n",
    "def pipeline(\n",
    "    in_bq_projectid: str,\n",
    "    in_bq_dataset: str,\n",
    "    in_bq_table: str,\n",
    "    in_pipeline_root: str,    \n",
    "    in_app_prefix: str,\n",
    "    in_vertexai_experiment_name: str, ### NEW PIPELINE PARAMETERS    This we need because our tasks will use Vertex AI SDK to log experiments\n",
    "    in_vertexai_region: str,          ### NEW PIPELINE PARAMETERS\n",
    "    in_vertexai_projectid: str,       ### NEW PIPELINE PARAMETERS\n",
    "    in_serving_machine_type: str,     ### NEW PIPELINE PARAMETERS    This we need because we need to let Vertex AI know how much compute we want to allocate for serving\n",
    "    in_serving_min_replica: int,      ### NEW PIPELINE PARAMETERS\n",
    "    in_serving_max_replica: int       ### NEW PIPELINE PARAMETERS\n",
    "):\n",
    "    stagingTask = stage(bq_projectid = in_bq_projectid,\n",
    "                                   bq_dataset   = in_bq_dataset, \n",
    "                                   bq_table     = in_bq_table)\n",
    "    \n",
    "    _user_id_column=\"synerise_client_id\"\n",
    "    _target_column=\"y_if_trans\"\n",
    "    _weight_column=\"weight\"\n",
    "    _excluded_columns = [_user_id_column, _weight_column]\n",
    "    \n",
    "    preprocessTask = preprocess(gcs_pipeline_root = in_pipeline_root,\n",
    "                                app_prefix = in_app_prefix, \n",
    "                                user_id_column = _user_id_column,\n",
    "                                target_column=_target_column,\n",
    "                                weight_column=_weight_column,\n",
    "                                excluded_columns = _excluded_columns,\n",
    "                                staged_bq_table = stagingTask.outputs[\"output_dataset\"]\n",
    "                               )\n",
    "    #### new comes here ---->\n",
    "    \"\"\"\n",
    "          staged_training_dataset: InputPath('staged_training_dataset'), \n",
    "          staged_validation_dataset: InputPath('staged_validation_dataset'), \n",
    "          staged_test_dataset: InputPath('staged_test_dataset'),\n",
    "          vertexai_experiment_name:str, \n",
    "          vertexai_region: str, \n",
    "          vertexai_projectid: str,\n",
    "          gcs_pipeline_root: str,\n",
    "          app_prefix: str, \n",
    "          user_id_column: str,\n",
    "          target_column: str,\n",
    "          weight_column: str,\n",
    "          excluded_columns: [],\n",
    "          output_model: Output[Model]\n",
    "    \"\"\"\n",
    "    \n",
    "    trainTask = train(staged_training_dataset = preprocessTask.outputs[\"staged_training_dataset\"],\n",
    "                      staged_validation_dataset = preprocessTask.outputs[\"staged_validation_dataset\"],\n",
    "                      staged_test_dataset = preprocessTask.outputs[\"staged_test_dataset\"],\n",
    "                      vertexai_experiment_name = in_vertexai_experiment_name,\n",
    "                      vertexai_region = in_vertexai_region,\n",
    "                      vertexai_projectid = in_vertexai_projectid,\n",
    "                      gcs_pipeline_root = in_pipeline_root,\n",
    "                      app_prefix = in_app_prefix,\n",
    "                      user_id_column = _user_id_column,\n",
    "                      target_column=_target_column,\n",
    "                      weight_column=_weight_column,\n",
    "                      excluded_columns = _excluded_columns\n",
    "                     )\n",
    "    \"\"\"\n",
    "          hmodel: Input[Model],\n",
    "          vertexai_experiment_name:str, \n",
    "          vertexai_region: str, \n",
    "          vertexai_projectid: str,\n",
    "          app_prefix: str,\n",
    "          serving_machine_type: str,\n",
    "          serving_min_replica: int,\n",
    "          serving_max_replica: int           \n",
    "         \n",
    "    \"\"\"\n",
    "    deployTask = simpledeploy(trainTask.outputs['output_model'],\n",
    "                             vertexai_experiment_name = in_vertexai_experiment_name,\n",
    "                             vertexai_region = in_vertexai_region,\n",
    "                             vertexai_projectid = in_vertexai_projectid,\n",
    "                             app_prefix = in_app_prefix,\n",
    "                             serving_machine_type = in_serving_machine_type,\n",
    "                             serving_min_replica = in_serving_min_replica,\n",
    "                             serving_max_replica = in_serving_max_replica\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f2aad91-e835-46a7-961e-4d5df0e7bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(\n",
    "   pipeline_func=pipeline,\n",
    "   package_path=dag_json_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f7eb8933-accd-4cf1-a0df-bc0221528eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/wf-kubeflow-e2e-mlops-20230221181847?project=339239659794\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/339239659794/locations/us-central1/pipelineJobs/wf-kubeflow-e2e-mlops-20230221181847\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = \"kfp_pipeline_e2e_mlops\",\n",
    "                             template_path = dag_json_filename,\n",
    "                             ##pipeline_root = PIPELINE_ROOT,\n",
    "                             parameter_values = PIPELINE_PARAMETERS, ## Make sure PIPELINE_PARAMETERS collection does not include parameters that are unknown to pipeline\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             labels = LABELS,\n",
    "                             project = VERTEXAI_PROJECT_ID,\n",
    "                             location = VERTEXAI_REGION)\n",
    "\n",
    "job.run(service_account=\"339239659794-compute@developer.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26228842-70d5-4899-86d9-20531fbb01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_run = aiplatform.ExperimentRun('my-run', experiment='my-experiment')\n",
    "my_job = aiplatform.PipelineJob(...)\n",
    "my_job.submit()\n",
    "my_run.log(my_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe90d90-45ab-45ad-b56f-a7e252598a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
